{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": [
    "# ML Model Optimization: Correlation-Guided Interaction Engineering\n",
    "## Model B — Lion (claude-churro-v10-p)\n",
    "\n",
    "**Expert**: Enzo Rodriguez | **Task ID**: TASK_11251 | **Date**: 2026-02-10\n",
    "\n",
    "---\n",
    "\n",
    "### What this notebook does\n",
    "\n",
    "This notebook replicates and **enhances** the R tidymodels/broom workflow in Python.\n",
    "The core idea: the model alone reaches a local equilibrium — the human element\n",
    "(guided by statistical evidence) decides which interaction terms to reintroduce.\n",
    "\n",
    "**Improvements over Model A baseline:**\n",
    "- Pearson correlations with **p-values** (statistical significance gating)\n",
    "- **Spearman** correlation to capture monotone non-linear relationships\n",
    "- **Mutual information** to catch any non-linear feature–target signal\n",
    "- **VIF** (Variance Inflation Factor) for multicollinearity — more rigorous than pairwise thresholds\n",
    "- Multiple interaction types scored simultaneously (multiplicative, ratio, polynomial)\n",
    "- **Ridge-regularized** model on enhanced feature set (prevents overfitting from added terms)\n",
    "- **Statsmodels OLS** tidy output: p-values, confidence intervals, F-statistic (broom-style)\n",
    "- Explicit **train/test gap monitoring** at every step\n",
    "- Marked **HUMAN DECISION CHECKPOINTs** where the analyst reviews evidence before proceeding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-setup",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, cross_validate, learning_curve\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-data",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Profile\n",
    "\n",
    "Using the **California Housing** dataset (sklearn). This replicates a real-world\n",
    "regression study with 8 numeric features and a continuous target (median house value).\n",
    "\n",
    "| Feature | Description |\n",
    "|---|---|\n",
    "| MedInc | Median income in block group |\n",
    "| HouseAge | Median house age |\n",
    "| AveRooms | Average number of rooms |\n",
    "| AveBedrms | Average number of bedrooms |\n",
    "| Population | Block group population |\n",
    "| AveOccup | Average household occupancy |\n",
    "| Latitude | Block group latitude |\n",
    "| Longitude | Block group longitude |\n",
    "| **MedHouseVal** | **Target: Median house value (\\$100k)** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing(as_frame=True)\n",
    "df_raw = housing.frame.copy()\n",
    "TARGET = 'MedHouseVal'\n",
    "\n",
    "print(f'Shape: {df_raw.shape}')\n",
    "print(f'Target: {TARGET}')\n",
    "df_raw.describe().T.style.background_gradient(cmap='Blues', subset=['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values and data types\n",
    "profile = pd.DataFrame({\n",
    "    'dtype': df_raw.dtypes,\n",
    "    'missing': df_raw.isnull().sum(),\n",
    "    'missing_pct': (df_raw.isnull().sum() / len(df_raw) * 100).round(2),\n",
    "    'unique': df_raw.nunique(),\n",
    "    'skewness': df_raw.skew().round(3)\n",
    "})\n",
    "print('Data Profile:')\n",
    "display(profile)\n",
    "print(f'\\nNo missing values: {df_raw.isnull().sum().sum() == 0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-outliers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR outlier removal on features (not target)\n",
    "FEATURES = [c for c in df_raw.columns if c != TARGET]\n",
    "df = df_raw.copy()\n",
    "\n",
    "outlier_flags = pd.Series(False, index=df.index)\n",
    "for col in FEATURES:\n",
    "    Q1, Q3 = df[col].quantile(0.25), df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = (df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)\n",
    "    outlier_flags |= mask\n",
    "\n",
    "removed = outlier_flags.sum()\n",
    "df = df[~outlier_flags].reset_index(drop=True)\n",
    "print(f'Removed {removed} outlier rows ({removed/len(df_raw)*100:.1f}%)')\n",
    "print(f'Cleaned dataset: {df.shape[0]} rows x {df.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-dist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions\n",
    "fig, axes = plt.subplots(3, 3, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "for i, col in enumerate(df.columns):\n",
    "    axes[i].hist(df[col], bins=40, edgecolor='black', alpha=0.7)\n",
    "    axes[i].set_title(col, fontsize=11, fontweight='bold')\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Count')\n",
    "    skew_val = df[col].skew()\n",
    "    axes[i].text(0.97, 0.95, f'skew={skew_val:.2f}',\n",
    "                 transform=axes[i].transAxes, ha='right', va='top', fontsize=9)\n",
    "plt.suptitle('Feature Distributions (after outlier removal)', fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/feature_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-corr",
   "metadata": {},
   "source": [
    "## 2. Enhanced Correlation Analysis\n",
    "\n",
    "**Model A used only Pearson correlation values.** \n",
    "Here we compute three complementary measures and gate on statistical significance:\n",
    "\n",
    "| Method | What it captures | When to prefer |\n",
    "|---|---|---|\n",
    "| Pearson + p-value | Linear relationships | Normal / symmetric data |\n",
    "| Spearman | Monotone non-linear | Skewed / ordinal data |\n",
    "| Mutual Information | Any non-linear signal | Always useful as a complement |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-pearson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation with p-values (feature vs target)\n",
    "pearson_rows = []\n",
    "for feat in FEATURES:\n",
    "    r, p = pearsonr(df[feat], df[TARGET])\n",
    "    pearson_rows.append({'feature': feat, 'pearson_r': r, 'pearson_abs': abs(r), 'pearson_p': p})\n",
    "\n",
    "pearson_df = pd.DataFrame(pearson_rows).sort_values('pearson_abs', ascending=False)\n",
    "pearson_df['significant_0.05'] = pearson_df['pearson_p'] < 0.05\n",
    "print('Pearson correlation with target (sorted by |r|):')\n",
    "display(pearson_df.set_index('feature'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-spearman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman correlation with p-values\n",
    "spearman_rows = []\n",
    "for feat in FEATURES:\n",
    "    rho, p = spearmanr(df[feat], df[TARGET])\n",
    "    spearman_rows.append({'feature': feat, 'spearman_rho': rho, 'spearman_abs': abs(rho), 'spearman_p': p})\n",
    "\n",
    "spearman_df = pd.DataFrame(spearman_rows).sort_values('spearman_abs', ascending=False)\n",
    "\n",
    "# Mutual information\n",
    "X_for_mi = df[FEATURES].values\n",
    "y_for_mi = df[TARGET].values\n",
    "mi_scores = mutual_info_regression(X_for_mi, y_for_mi, random_state=RANDOM_STATE)\n",
    "mi_df = pd.DataFrame({'feature': FEATURES, 'mutual_info': mi_scores}).sort_values('mutual_info', ascending=False)\n",
    "\n",
    "# Merge all three into one summary table\n",
    "corr_summary = (\n",
    "    pearson_df[['feature', 'pearson_r', 'pearson_abs', 'pearson_p']]\n",
    "    .merge(spearman_df[['feature', 'spearman_rho', 'spearman_abs']], on='feature')\n",
    "    .merge(mi_df, on='feature')\n",
    "    .sort_values('mutual_info', ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "corr_summary['rank_pearson'] = corr_summary['pearson_abs'].rank(ascending=False).astype(int)\n",
    "corr_summary['rank_spearman'] = corr_summary['spearman_abs'].rank(ascending=False).astype(int)\n",
    "corr_summary['rank_mi'] = corr_summary['mutual_info'].rank(ascending=False).astype(int)\n",
    "\n",
    "print('Unified correlation summary (all three methods):')\n",
    "display(corr_summary.set_index('feature')[\n",
    "    ['pearson_r', 'pearson_p', 'spearman_rho', 'mutual_info',\n",
    "     'rank_pearson', 'rank_spearman', 'rank_mi']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-corr-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side bar chart comparing all three methods\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "def bar_corr(ax, series, labels, title, color):\n",
    "    sorted_idx = np.argsort(np.abs(series))[::-1]\n",
    "    vals = np.array(series)[sorted_idx]\n",
    "    labs = np.array(labels)[sorted_idx]\n",
    "    colors = ['#e74c3c' if v < 0 else color for v in vals]\n",
    "    ax.barh(labs, vals, color=colors, alpha=0.8, edgecolor='k', linewidth=0.4)\n",
    "    ax.axvline(0, color='black', linewidth=0.8)\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "bar_corr(axes[0], corr_summary['pearson_r'].values,\n",
    "         corr_summary['feature'].values, 'Pearson r', '#3498db')\n",
    "bar_corr(axes[1], corr_summary['spearman_rho'].values,\n",
    "         corr_summary['feature'].values, 'Spearman rho', '#2ecc71')\n",
    "bar_corr(axes[2], corr_summary['mutual_info'].values,\n",
    "         corr_summary['feature'].values, 'Mutual Information', '#9b59b6')\n",
    "\n",
    "plt.suptitle(f'Feature Correlations with {TARGET}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/correlation_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pairwise correlation heatmap with annotations\n",
    "corr_matrix = df[FEATURES].corr(method='pearson')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(\n",
    "    corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "    center=0, square=True, linewidths=0.5, ax=ax,\n",
    "    cbar_kws={'shrink': 0.8}\n",
    ")\n",
    "ax.set_title('Feature-Feature Pearson Correlation (lower triangle)', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/correlation_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-vif",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance Inflation Factor (VIF) — detects multicollinearity more rigorously than pairwise thresholds\n",
    "# VIF > 10 is a strong multicollinearity signal; VIF > 5 is moderate\n",
    "X_vif = df[FEATURES].copy()\n",
    "X_vif_scaled = StandardScaler().fit_transform(X_vif)\n",
    "X_vif_sm = pd.DataFrame(X_vif_scaled, columns=FEATURES)\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = FEATURES\n",
    "vif_data['VIF'] = [variance_inflation_factor(X_vif_sm.values, i) for i in range(len(FEATURES))]\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False).reset_index(drop=True)\n",
    "vif_data['multicollinearity'] = pd.cut(\n",
    "    vif_data['VIF'], bins=[0, 5, 10, np.inf], labels=['Low', 'Moderate', 'High']\n",
    ")\n",
    "\n",
    "print('Variance Inflation Factor (VIF):')\n",
    "print('  VIF < 5   = Low multicollinearity (safe to include)')\n",
    "print('  VIF 5-10  = Moderate — monitor closely')\n",
    "print('  VIF > 10  = High — consider removing or combining\\n')\n",
    "display(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-interaction-candidates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction candidate scoring (enhanced over Model A)\n",
    "# Model A score = (|r1_target| + |r2_target|) * |r12_inter_feature|\n",
    "# Model B adds mutual information weight: gives extra credit when MI is high\n",
    "\n",
    "# Gate on significance: only use features with p < 0.05\n",
    "sig_features = pearson_df[pearson_df['pearson_p'] < 0.05]['feature'].tolist()\n",
    "print(f'Features with significant Pearson correlation (p < 0.05): {sig_features}\\n')\n",
    "\n",
    "# Build lookup dicts\n",
    "pearson_lookup = dict(zip(pearson_df['feature'], pearson_df['pearson_abs']))\n",
    "mi_lookup = dict(zip(mi_df['feature'], mi_df['mutual_info']))\n",
    "\n",
    "candidates = []\n",
    "for i, f1 in enumerate(sig_features):\n",
    "    for f2 in sig_features[i+1:]:\n",
    "        inter_corr = abs(corr_matrix.loc[f1, f2])\n",
    "        # Include pairs with moderate inter-feature correlation (not too low, not too high)\n",
    "        if 0.05 <= inter_corr <= 0.80:\n",
    "            r1 = pearson_lookup.get(f1, 0)\n",
    "            r2 = pearson_lookup.get(f2, 0)\n",
    "            mi1 = mi_lookup.get(f1, 0)\n",
    "            mi2 = mi_lookup.get(f2, 0)\n",
    "            # Enhanced score: linear + MI components\n",
    "            linear_score = (r1 + r2) * inter_corr\n",
    "            mi_score = (mi1 + mi2) * inter_corr\n",
    "            combined_score = 0.5 * linear_score + 0.5 * mi_score\n",
    "            candidates.append({\n",
    "                'feature_1': f1, 'feature_2': f2,\n",
    "                'f1_pearson': round(r1, 4), 'f2_pearson': round(r2, 4),\n",
    "                'f1_MI': round(mi1, 4), 'f2_MI': round(mi2, 4),\n",
    "                'inter_feature_corr': round(inter_corr, 4),\n",
    "                'linear_score': round(linear_score, 4),\n",
    "                'mi_score': round(mi_score, 4),\n",
    "                'combined_score': round(combined_score, 4)\n",
    "            })\n",
    "\n",
    "candidates_df = pd.DataFrame(candidates).sort_values('combined_score', ascending=False).reset_index(drop=True)\n",
    "print(f'Total interaction candidates: {len(candidates_df)}')\n",
    "print('\\nTop 15 Interaction Candidates (Model B enhanced scoring):')\n",
    "display(candidates_df.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-checkpoint1",
   "metadata": {},
   "source": [
    "---\n",
    "## HUMAN DECISION CHECKPOINT 1: Select Interaction Candidates\n",
    "\n",
    "**Review the table above before proceeding.**\n",
    "\n",
    "Ask yourself:\n",
    "1. Do any of the top pairs have domain-level justification?\n",
    "   - `MedInc × AveRooms` — wealthier blocks tend to have more spacious homes; the *combination* may better explain value\n",
    "   - `Latitude × Longitude` — geographic location as a 2D interaction captures region (e.g., coastal vs inland)\n",
    "   - `MedInc × HouseAge` — new homes in rich areas vs. old homes in rich areas behave differently\n",
    "   - `Population × AveOccup` — density effects\n",
    "2. Does the `inter_feature_corr` suggest the two features are **not** redundant (VIF risk)?\n",
    "3. Is the `mi_score` consistent with the `linear_score`? A large gap suggests non-linearity worth capturing.\n",
    "\n",
    "**The cell below lets you configure which interactions to carry forward.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-analyst-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── ANALYST CONFIGURATION ──────────────────────────────────────────────\n",
    "# Edit TOP_N_AUTO to automatically take the top N by combined_score,\n",
    "# OR override by listing specific pairs in MANUAL_PAIRS.\n",
    "# Set MANUAL_PAIRS = [] to use automatic selection only.\n",
    "TOP_N_AUTO = 12\n",
    "MANUAL_PAIRS = []   # e.g. [('MedInc', 'Latitude'), ('HouseAge', 'AveRooms')]\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "\n",
    "auto_pairs = [\n",
    "    (row['feature_1'], row['feature_2'])\n",
    "    for _, row in candidates_df.head(TOP_N_AUTO).iterrows()\n",
    "]\n",
    "all_pairs = list(dict.fromkeys(auto_pairs + MANUAL_PAIRS))  # preserve order, deduplicate\n",
    "\n",
    "print(f'Proceeding with {len(all_pairs)} interaction pairs:')\n",
    "for p in all_pairs:\n",
    "    print(f'  {p[0]} x {p[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-engineering",
   "metadata": {},
   "source": [
    "## 3. Interaction Engineering\n",
    "\n",
    "We create **three interaction types** for each selected pair, then let cross-validation\n",
    "decide which form actually helps. This avoids assuming multiplicative is always best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-create-interactions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_interactions(df, pairs):\n",
    "    \"\"\"Create multiplicative, ratio, and difference interactions for all pairs.\"\"\"\n",
    "    terms = {}\n",
    "    eps = 1e-8\n",
    "    for f1, f2 in pairs:\n",
    "        terms[f'{f1}_x_{f2}']       = df[f1] * df[f2]            # multiplicative\n",
    "        terms[f'{f1}_div_{f2}']     = df[f1] / (df[f2] + eps)    # ratio\n",
    "        terms[f'{f1}_minus_{f2}']   = df[f1] - df[f2]            # difference\n",
    "    # Polynomial terms for top-MI features\n",
    "    top_mi_features = mi_df['feature'].head(3).tolist()\n",
    "    for feat in top_mi_features:\n",
    "        terms[f'{feat}_sq'] = df[feat] ** 2\n",
    "    return pd.DataFrame(terms, index=df.index)\n",
    "\n",
    "interactions_df = make_interactions(df, all_pairs)\n",
    "print(f'Created {interactions_df.shape[1]} raw interaction terms:')\n",
    "for col in interactions_df.columns:\n",
    "    print(f'  {col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-interaction-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each interaction term via 5-fold CV\n",
    "# Metric: improvement in R2 over the baseline (no interactions)\n",
    "X_base = df[FEATURES].copy()\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_base_scaled = scaler.fit_transform(X_base)\n",
    "\n",
    "rf_eval = RandomForestRegressor(n_estimators=50, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "baseline_cv = cross_val_score(rf_eval, X_base_scaled, y, cv=5, scoring='r2')\n",
    "baseline_mean = baseline_cv.mean()\n",
    "print(f'Baseline CV R2: {baseline_mean:.4f} (std {baseline_cv.std():.4f})')\n",
    "\n",
    "importance_rows = []\n",
    "for col in interactions_df.columns:\n",
    "    X_with = np.hstack([X_base_scaled, interactions_df[[col]].values])\n",
    "    scores = cross_val_score(rf_eval, X_with, y, cv=5, scoring='r2')\n",
    "    improvement = scores.mean() - baseline_mean\n",
    "    importance_rows.append({\n",
    "        'term': col,\n",
    "        'cv_r2': round(scores.mean(), 5),\n",
    "        'cv_std': round(scores.std(), 5),\n",
    "        'improvement': round(improvement, 5),\n",
    "        'improvement_pct': round(improvement / abs(baseline_mean) * 100, 3)\n",
    "    })\n",
    "\n",
    "importance_result = pd.DataFrame(importance_rows).sort_values('improvement', ascending=False).reset_index(drop=True)\n",
    "print(f'\\nInteraction Term Importance (sorted by R2 improvement):')\n",
    "display(importance_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-interaction-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize interaction importance\n",
    "fig, ax = plt.subplots(figsize=(12, max(6, len(importance_result) * 0.35)))\n",
    "colors = ['#27ae60' if v >= 0 else '#e74c3c' for v in importance_result['improvement']]\n",
    "ax.barh(importance_result['term'][::-1], importance_result['improvement'][::-1],\n",
    "        color=colors[::-1], alpha=0.85, edgecolor='k', linewidth=0.4)\n",
    "ax.axvline(0, color='black', linewidth=1)\n",
    "ax.set_xlabel('R² Improvement over Baseline', fontsize=12)\n",
    "ax.set_title('Interaction Term R² Improvement (5-fold CV)', fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/interaction_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-select-interactions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select interactions with positive improvement\n",
    "# Threshold: improvement must be > 0 (strictly beneficial)\n",
    "selected_terms = importance_result[importance_result['improvement'] > 0]['term'].tolist()\n",
    "print(f'Interactions with positive R2 improvement: {len(selected_terms)}')\n",
    "print(f'Interactions that hurt performance: {(importance_result[\"improvement\"] < 0).sum()}')\n",
    "print(f'\\nSelected terms: {selected_terms}')\n",
    "\n",
    "# Build enhanced dataset: original features + selected interactions\n",
    "df_enhanced = pd.concat([df[FEATURES + [TARGET]], interactions_df[selected_terms]], axis=1)\n",
    "print(f'\\nEnhanced dataset: {df_enhanced.shape[1] - 1} features ({len(FEATURES)} original + {len(selected_terms)} interactions)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-overfit",
   "metadata": {},
   "source": [
    "## 4. Overfitting Detection: Train vs. Test Gap\n",
    "\n",
    "Before committing to the enhanced feature set, we explicitly check whether adding\n",
    "interactions inflates training performance without improving generalization.\n",
    "\n",
    "**Rule of thumb**: if Train R² >> Test R², we are overfitting — add Ridge regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-overfit-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overfit_check(data, target, label, scaler=None, random_state=42):\n",
    "    \"\"\"Return train/test R2 for Random Forest and a gap indicator.\"\"\"\n",
    "    feats = [c for c in data.columns if c != target]\n",
    "    X = data[feats].values\n",
    "    y_vals = data[target].values\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y_vals, test_size=0.2, random_state=random_state)\n",
    "    if scaler:\n",
    "        sc = StandardScaler()\n",
    "        X_tr = sc.fit_transform(X_tr)\n",
    "        X_te = sc.transform(X_te)\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=random_state, n_jobs=-1)\n",
    "    rf.fit(X_tr, y_tr)\n",
    "    train_r2 = r2_score(y_tr, rf.predict(X_tr))\n",
    "    test_r2  = r2_score(y_te, rf.predict(X_te))\n",
    "    gap = train_r2 - test_r2\n",
    "    print(f'[{label}] Train R2: {train_r2:.4f} | Test R2: {test_r2:.4f} | Gap: {gap:.4f}')\n",
    "    return {'label': label, 'train_r2': train_r2, 'test_r2': test_r2, 'gap': gap}\n",
    "\n",
    "gap_rows = []\n",
    "gap_rows.append(overfit_check(df[FEATURES + [TARGET]], TARGET, 'Baseline (no interactions)'))\n",
    "gap_rows.append(overfit_check(df_enhanced, TARGET, f'Enhanced (+{len(selected_terms)} interactions)'))\n",
    "\n",
    "gap_df = pd.DataFrame(gap_rows)\n",
    "print()\n",
    "display(gap_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-checkpoint2",
   "metadata": {},
   "source": [
    "---\n",
    "## HUMAN DECISION CHECKPOINT 2: Regularization Strategy\n",
    "\n",
    "**Review the Train/Test Gap above before configuring models.**\n",
    "\n",
    "| Gap size | Recommendation |\n",
    "|---|---|\n",
    "| < 0.02 | No overfitting — Random Forest with interactions is fine |\n",
    "| 0.02 – 0.08 | Mild overfitting — add Ridge alongside RF |\n",
    "| > 0.08 | Overfitting — use Ridge/Lasso primarily; reduce interaction count |\n",
    "\n",
    "The model suite below trains Ridge and ElasticNet alongside Random Forest, so all\n",
    "scenarios are covered. The comparison table will tell you which approach wins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-training",
   "metadata": {},
   "source": [
    "## 5. Model Training: Baseline vs Enhanced vs Regularized\n",
    "\n",
    "All models are trained with the **same 80/20 split** and **5-fold cross-validation**.\n",
    "Scaling is applied uniformly (fitted on train only, applied to test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-train-all",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(data, target, model, label, X_tr=None, X_te=None, y_tr=None, y_te=None, cv=5):\n",
    "    \"\"\"Train a model, run CV, return a result dict.\"\"\"\n",
    "    feats = [c for c in data.columns if c != target]\n",
    "    X = data[feats].values\n",
    "    y_vals = data[target].values\n",
    "\n",
    "    if X_tr is None:\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(X, y_vals, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    X_tr_sc = sc.fit_transform(X_tr)\n",
    "    X_te_sc  = sc.transform(X_te)\n",
    "\n",
    "    cv_scores = cross_val_score(model, X_tr_sc, y_tr, cv=cv, scoring='r2')\n",
    "\n",
    "    model.fit(X_tr_sc, y_tr)\n",
    "    y_train_pred = model.predict(X_tr_sc)\n",
    "    y_test_pred  = model.predict(X_te_sc)\n",
    "\n",
    "    return {\n",
    "        'label': label,\n",
    "        'n_features': len(feats),\n",
    "        'cv_r2_mean': round(cv_scores.mean(), 4),\n",
    "        'cv_r2_std':  round(cv_scores.std(), 4),\n",
    "        'train_r2':   round(r2_score(y_tr, y_train_pred), 4),\n",
    "        'test_r2':    round(r2_score(y_te, y_test_pred), 4),\n",
    "        'test_rmse':  round(np.sqrt(mean_squared_error(y_te, y_test_pred)), 4),\n",
    "        'test_mae':   round(mean_absolute_error(y_te, y_test_pred), 4),\n",
    "        'gap':        round(r2_score(y_tr, y_train_pred) - r2_score(y_te, y_test_pred), 4),\n",
    "        'model_obj':  model,\n",
    "        'y_test':     y_te,\n",
    "        'y_pred':     y_test_pred,\n",
    "        'y_train':    y_tr,\n",
    "        'y_train_pred': y_train_pred\n",
    "    }\n",
    "\n",
    "results = []\n",
    "data_base = df[FEATURES + [TARGET]]\n",
    "data_enh  = df_enhanced\n",
    "\n",
    "# Baseline models\n",
    "results.append(train_and_evaluate(data_base, TARGET, LinearRegression(), 'Baseline — Linear Regression'))\n",
    "results.append(train_and_evaluate(data_base, TARGET, Ridge(alpha=1.0),   'Baseline — Ridge'))\n",
    "results.append(train_and_evaluate(data_base, TARGET, RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1), 'Baseline — Random Forest'))\n",
    "results.append(train_and_evaluate(data_base, TARGET, GradientBoostingRegressor(n_estimators=100, random_state=RANDOM_STATE), 'Baseline — Gradient Boosting'))\n",
    "\n",
    "# Enhanced models (+interactions, no regularization)\n",
    "results.append(train_and_evaluate(data_enh, TARGET, LinearRegression(), 'Enhanced — Linear Regression'))\n",
    "results.append(train_and_evaluate(data_enh, TARGET, Ridge(alpha=1.0),   'Enhanced — Ridge'))\n",
    "results.append(train_and_evaluate(data_enh, TARGET, Ridge(alpha=10.0),  'Enhanced — Ridge (alpha=10)'))\n",
    "results.append(train_and_evaluate(data_enh, TARGET, Lasso(alpha=0.01),  'Enhanced — Lasso'))\n",
    "results.append(train_and_evaluate(data_enh, TARGET, ElasticNet(alpha=0.01, l1_ratio=0.5), 'Enhanced — ElasticNet'))\n",
    "results.append(train_and_evaluate(data_enh, TARGET, RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1), 'Enhanced — Random Forest'))\n",
    "results.append(train_and_evaluate(data_enh, TARGET, GradientBoostingRegressor(n_estimators=100, random_state=RANDOM_STATE), 'Enhanced — Gradient Boosting'))\n",
    "\n",
    "print('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-compare-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison table\n",
    "compare_cols = ['label', 'n_features', 'cv_r2_mean', 'cv_r2_std', 'train_r2', 'test_r2', 'test_rmse', 'test_mae', 'gap']\n",
    "compare_df = pd.DataFrame([{k: r[k] for k in compare_cols} for r in results])\n",
    "compare_df = compare_df.sort_values('test_r2', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print('Model Comparison (sorted by Test R2):')\n",
    "display(\n",
    "    compare_df.style\n",
    "    .background_gradient(subset=['test_r2', 'cv_r2_mean'], cmap='Greens')\n",
    "    .background_gradient(subset=['gap'], cmap='Reds')\n",
    "    .format({'cv_r2_mean': '{:.4f}', 'test_r2': '{:.4f}', 'gap': '{:.4f}'})\n",
    ")\n",
    "\n",
    "best = compare_df.iloc[0]\n",
    "print(f'\\nBest model: {best[\"label\"]}')\n",
    "print(f'  Test R2: {best[\"test_r2\"]:.4f} | CV R2: {best[\"cv_r2_mean\"]:.4f} | Gap: {best[\"gap\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-compare-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison: Test R2 with Train/Test gap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax = axes[0]\n",
    "labels_short = [r['label'].replace('Baseline — ', 'B: ').replace('Enhanced — ', 'E: ') for r in results]\n",
    "test_r2s  = [r['test_r2']  for r in results]\n",
    "train_r2s = [r['train_r2'] for r in results]\n",
    "x_pos = np.arange(len(results))\n",
    "ax.barh(x_pos, test_r2s, alpha=0.8, color='#3498db', label='Test R2', edgecolor='k', linewidth=0.4)\n",
    "ax.barh(x_pos, train_r2s, alpha=0.3, color='#e74c3c', label='Train R2', edgecolor='k', linewidth=0.3)\n",
    "ax.set_yticks(x_pos)\n",
    "ax.set_yticklabels(labels_short, fontsize=9)\n",
    "ax.set_xlabel('R²')\n",
    "ax.set_title('Train vs Test R² (all models)', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "ax2 = axes[1]\n",
    "gaps = [r['gap'] for r in results]\n",
    "gap_colors = ['#e74c3c' if g > 0.05 else '#f39c12' if g > 0.02 else '#27ae60' for g in gaps]\n",
    "ax2.barh(x_pos, gaps, color=gap_colors, alpha=0.8, edgecolor='k', linewidth=0.4)\n",
    "ax2.set_yticks(x_pos)\n",
    "ax2.set_yticklabels(labels_short, fontsize=9)\n",
    "ax2.axvline(0.02, color='orange', linestyle='--', linewidth=1, label='Mild threshold (0.02)')\n",
    "ax2.axvline(0.08, color='red',    linestyle='--', linewidth=1, label='High threshold (0.08)')\n",
    "ax2.set_xlabel('Train R² − Test R²')\n",
    "ax2.set_title('Overfitting Gap by Model', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-ols",
   "metadata": {},
   "source": [
    "## 6. Statsmodels OLS — Broom-Style Tidy Output\n",
    "\n",
    "Random Forest gives strong predictions but no interpretability. Here we fit\n",
    "**Ordinary Least Squares** on the enhanced feature set to get:\n",
    "- **Coefficients** with p-values and confidence intervals\n",
    "- **F-statistic** (overall model significance)\n",
    "- **AIC / BIC** (model selection criteria)\n",
    "\n",
    "This is the Python equivalent of R's `broom::tidy()` and `broom::glance()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-ols",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS on the enhanced feature set (scaled)\n",
    "enh_feats = [c for c in df_enhanced.columns if c != TARGET]\n",
    "X_enh = df_enhanced[enh_feats].values\n",
    "y_vals = df_enhanced[TARGET].values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_enh, y_vals, test_size=0.2, random_state=RANDOM_STATE)\n",
    "sc_ols = StandardScaler()\n",
    "X_tr_sc = sc_ols.fit_transform(X_tr)\n",
    "X_tr_sm = sm.add_constant(X_tr_sc)   # add intercept\n",
    "\n",
    "ols_model = sm.OLS(y_tr, X_tr_sm).fit()\n",
    "print(ols_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-ols-tidy",
   "metadata": {},
   "outputs": [],
   "source": "# broom::tidy() equivalent — coefficient table\ncol_names_ols = ['const'] + enh_feats\nci = ols_model.conf_int()  # numpy array (n_params, 2): col 0=lower, col 1=upper\ntidy_df = pd.DataFrame({\n    'term':        col_names_ols,\n    'estimate':    list(ols_model.params),\n    'std_error':   list(ols_model.bse),\n    't_statistic': list(ols_model.tvalues),\n    'p_value':     list(ols_model.pvalues),\n    'ci_lower':    list(ci[:, 0]),\n    'ci_upper':    list(ci[:, 1])\n})\ntidy_df['significant'] = tidy_df['p_value'] < 0.05\n\nprint('broom::tidy() — Coefficient table:')\ndisplay(\n    tidy_df.sort_values('p_value').style\n    .background_gradient(subset=['p_value'], cmap='RdYlGn_r')\n    .format({'estimate': '{:.4f}', 'p_value': '{:.4e}', 'ci_lower': '{:.4f}', 'ci_upper': '{:.4f}'})\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-ols-glance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# broom::glance() equivalent — model-level statistics\n",
    "glance_df = pd.DataFrame([{\n",
    "    'r_squared':      round(ols_model.rsquared, 4),\n",
    "    'adj_r_squared':  round(ols_model.rsquared_adj, 4),\n",
    "    'f_statistic':    round(ols_model.fvalue, 2),\n",
    "    'f_pvalue':       round(ols_model.f_pvalue, 6),\n",
    "    'aic':            round(ols_model.aic, 2),\n",
    "    'bic':            round(ols_model.bic, 2),\n",
    "    'n_obs':          int(ols_model.nobs),\n",
    "    'df_model':       int(ols_model.df_model),\n",
    "    'df_residuals':   int(ols_model.df_resid)\n",
    "}])\n",
    "print('broom::glance() — Model-level statistics:')\n",
    "display(glance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-ols-coef-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient plot with 95% CI (exclude intercept)\n",
    "coef_plot = tidy_df[tidy_df['term'] != 'const'].sort_values('estimate')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, max(6, len(coef_plot) * 0.4)))\n",
    "colors = ['#27ae60' if s else '#95a5a6' for s in coef_plot['significant']]\n",
    "ax.barh(coef_plot['term'], coef_plot['estimate'], color=colors, alpha=0.8, edgecolor='k', linewidth=0.3)\n",
    "ax.errorbar(\n",
    "    coef_plot['estimate'], coef_plot['term'],\n",
    "    xerr=[\n",
    "        coef_plot['estimate'] - coef_plot['ci_lower'],\n",
    "        coef_plot['ci_upper'] - coef_plot['estimate']\n",
    "    ],\n",
    "    fmt='none', color='black', capsize=3, linewidth=1\n",
    ")\n",
    "ax.axvline(0, color='black', linewidth=1)\n",
    "ax.set_xlabel('OLS Coefficient (standardized features)', fontsize=11)\n",
    "ax.set_title('OLS Coefficients with 95% CI\\n(green = p < 0.05)', fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/ols_coefficients.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-eval",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation & Residual Diagnostics\n",
    "\n",
    "Full diagnostic plots for the best-performing enhanced model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-find-best",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best enhanced model from results list\n",
    "best_result = max(\n",
    "    [r for r in results if r['label'].startswith('Enhanced')],\n",
    "    key=lambda r: r['test_r2']\n",
    ")\n",
    "print(f'Best enhanced model: {best_result[\"label\"]}')\n",
    "print(f'  Test R2:  {best_result[\"test_r2\"]:.4f}')\n",
    "print(f'  Test RMSE: {best_result[\"test_rmse\"]:.4f}')\n",
    "print(f'  Test MAE:  {best_result[\"test_mae\"]:.4f}')\n",
    "print(f'  Overfit gap: {best_result[\"gap\"]:.4f}')\n",
    "\n",
    "y_test  = best_result['y_test']\n",
    "y_pred  = best_result['y_pred']\n",
    "residuals = y_test - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-diag-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-panel residual diagnostic (mirrors R's plot(lm(...)))\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Predicted vs Actual\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(y_test, y_pred, alpha=0.4, edgecolors='k', linewidth=0.3, s=12)\n",
    "mn, mx = min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())\n",
    "ax.plot([mn, mx], [mn, mx], 'r--', lw=2, label='Perfect prediction')\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "ax.text(0.05, 0.95, f'R² = {r2:.4f}', transform=ax.transAxes, va='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "ax.set_xlabel('Actual'); ax.set_ylabel('Predicted')\n",
    "ax.set_title('Predicted vs Actual', fontweight='bold'); ax.legend(); ax.grid(alpha=0.3)\n",
    "\n",
    "# 2. Residuals vs Fitted\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(y_pred, residuals, alpha=0.4, edgecolors='k', linewidth=0.3, s=12)\n",
    "ax.axhline(0, color='red', lw=2, linestyle='--')\n",
    "ax.set_xlabel('Fitted Values'); ax.set_ylabel('Residuals')\n",
    "ax.set_title('Residuals vs Fitted', fontweight='bold'); ax.grid(alpha=0.3)\n",
    "\n",
    "# 3. Q-Q plot\n",
    "ax = axes[1, 0]\n",
    "stats.probplot(residuals, dist='norm', plot=ax)\n",
    "ax.set_title('Normal Q-Q Plot', fontweight='bold'); ax.grid(alpha=0.3)\n",
    "\n",
    "# 4. Scale-Location\n",
    "ax = axes[1, 1]\n",
    "std_resid = residuals / residuals.std()\n",
    "ax.scatter(y_pred, np.sqrt(np.abs(std_resid)), alpha=0.4, edgecolors='k', linewidth=0.3, s=12)\n",
    "ax.set_xlabel('Fitted Values'); ax.set_ylabel('sqrt(|Std Residuals|)')\n",
    "ax.set_title('Scale-Location', fontweight='bold'); ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Residual Diagnostics — {best_result[\"label\"]}', fontsize=13, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/residual_diagnostics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-resid-tests",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical tests on residuals\n",
    "shapiro_stat, shapiro_p = stats.shapiro(residuals[:5000])  # limit to 5000 for Shapiro\n",
    "dw_stat = np.sum(np.diff(residuals)**2) / np.sum(residuals**2)\n",
    "\n",
    "print('Residual Assumption Tests:')\n",
    "print(f'  Shapiro-Wilk normality: stat={shapiro_stat:.4f}, p={shapiro_p:.4e}')\n",
    "print(f'    Residuals are normal: {shapiro_p > 0.05}')\n",
    "print(f'  Durbin-Watson: {dw_stat:.4f}')\n",
    "if dw_stat < 1.5:\n",
    "    print('    Positive autocorrelation detected')\n",
    "elif dw_stat > 2.5:\n",
    "    print('    Negative autocorrelation detected')\n",
    "else:\n",
    "    print('    No significant autocorrelation')\n",
    "\n",
    "print(f'\\nResidual Stats:')\n",
    "print(f'  Mean:     {residuals.mean():.6f}  (should be ~0)')\n",
    "print(f'  Std Dev:  {residuals.std():.4f}')\n",
    "print(f'  Skewness: {stats.skew(residuals):.4f}  (should be ~0)')\n",
    "print(f'  Kurtosis: {stats.kurtosis(residuals):.4f}  (should be ~0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-importance",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis\n",
    "\n",
    "Which of the added interaction terms actually ended up being important to the best tree model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_obj = best_result['model_obj']\n",
    "\n",
    "if hasattr(best_model_obj, 'feature_importances_'):\n",
    "    enh_feat_names = [c for c in df_enhanced.columns if c != TARGET]\n",
    "    fi_df = pd.DataFrame({\n",
    "        'feature':    enh_feat_names,\n",
    "        'importance': best_model_obj.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    fi_df['is_interaction'] = fi_df['feature'].isin(selected_terms)\n",
    "\n",
    "    print(f'Top 20 features by importance:')\n",
    "    display(fi_df.head(20))\n",
    "\n",
    "    # Pie chart: original vs interaction importance share\n",
    "    orig_share  = fi_df[~fi_df['is_interaction']]['importance'].sum()\n",
    "    inter_share = fi_df[ fi_df['is_interaction']]['importance'].sum()\n",
    "    print(f'\\nImportance share — Original features: {orig_share:.1%} | Interaction terms: {inter_share:.1%}')\nelse:\n",
    "    print('Model does not expose feature importances — use OLS tidy output instead.')\n",
    "    fi_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-fi-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fi_df is not None:\n",
    "    fig, ax = plt.subplots(figsize=(11, 7))\n",
    "    top20 = fi_df.head(20)\n",
    "    colors = ['#e67e22' if inter else '#3498db' for inter in top20['is_interaction']]\n",
    "    ax.barh(top20['feature'][::-1], top20['importance'][::-1],\n",
    "            color=colors[::-1], alpha=0.85, edgecolor='k', linewidth=0.4)\n",
    "    ax.set_xlabel('Feature Importance', fontsize=12)\n",
    "    ax.set_title(f'Top 20 Feature Importances\\n({best_result[\"label\"]})', fontsize=12, fontweight='bold')\n",
    "    # Legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elems = [Patch(color='#3498db', alpha=0.85, label='Original feature'),\n",
    "                    Patch(color='#e67e22', alpha=0.85, label='Interaction term')]\n",
    "    ax.legend(handles=legend_elems, fontsize=10)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-lcurve",
   "metadata": {},
   "source": [
    "## 9. Learning Curves\n",
    "\n",
    "Learning curves show whether the model benefits from more data (underfitting)\n",
    "or has already plateaued (overfitting). Comparing baseline vs enhanced curves\n",
    "confirms that the added interactions generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-learning-curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(ax, X, y, model, title, color='#3498db'):\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X, y, cv=4,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 8),\n",
    "        scoring='r2', n_jobs=-1\n",
    "    )\n",
    "    tr_mean, tr_std = train_scores.mean(axis=1), train_scores.std(axis=1)\n",
    "    va_mean, va_std = val_scores.mean(axis=1), val_scores.std(axis=1)\n",
    "    ax.plot(train_sizes, tr_mean, 'o-', color=color, label='Train')\n",
    "    ax.fill_between(train_sizes, tr_mean - tr_std, tr_mean + tr_std, alpha=0.15, color=color)\n",
    "    ax.plot(train_sizes, va_mean, 's--', color='#e74c3c', label='Validation')\n",
    "    ax.fill_between(train_sizes, va_mean - va_std, va_mean + va_std, alpha=0.15, color='#e74c3c')\n",
    "    ax.set_xlabel('Training Set Size'); ax.set_ylabel('R²')\n",
    "    ax.set_title(title, fontweight='bold'); ax.legend(); ax.grid(alpha=0.3)\n",
    "\n",
    "rf_base = RandomForestRegressor(n_estimators=50, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf_enh  = RandomForestRegressor(n_estimators=50, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "sc_lc = StandardScaler()\n",
    "X_base_lc = sc_lc.fit_transform(df[FEATURES].values)\n",
    "X_enh_lc  = StandardScaler().fit_transform(df_enhanced[[c for c in df_enhanced.columns if c != TARGET]].values)\n",
    "y_lc = df[TARGET].values\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "plot_learning_curve(axes[0], X_base_lc, y_lc, rf_base, 'Baseline (no interactions)', '#3498db')\n",
    "plot_learning_curve(axes[1], X_enh_lc,  y_lc, rf_enh,  f'Enhanced (+{len(selected_terms)} interactions)', '#2ecc71')\n",
    "plt.suptitle('Learning Curves: Random Forest', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/learning_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-summary",
   "metadata": {},
   "source": [
    "## 10. Summary & Recommendations\n",
    "\n",
    "This cell auto-generates a summary from the results computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline best\n",
    "best_base = max([r for r in results if r['label'].startswith('Baseline')], key=lambda r: r['test_r2'])\n",
    "best_enh  = max([r for r in results if r['label'].startswith('Enhanced')], key=lambda r: r['test_r2'])\n",
    "\n",
    "r2_lift = best_enh['test_r2'] - best_base['test_r2']\n",
    "rmse_reduction = best_base['test_rmse'] - best_enh['test_rmse']\n",
    "\n",
    "sig_interactions = tidy_df[tidy_df['significant'] & tidy_df['term'].isin(selected_terms)]['term'].tolist()\n",
    "\n",
    "print('=' * 65)\n",
    "print('  ML OPTIMIZATION SUMMARY — Model B (Lion)')\n",
    "print('=' * 65)\n",
    "print(f'\\nDataset:  California Housing  ({df.shape[0]} samples, {len(FEATURES)} features)')\n",
    "print(f'Target:   {TARGET}')\n",
    "\n",
    "print(f'\\nCorrelation Analysis:')\n",
    "print(f'  Significant features (p < 0.05):  {len(sig_features)}/{len(FEATURES)}')\n",
    "print(f'  Features with high VIF (>10):     {(vif_data[\"VIF\"] > 10).sum()}')\n",
    "print(f'  Interaction candidates identified: {len(candidates_df)}')\n",
    "\n",
    "print(f'\\nInteraction Engineering:')\n",
    "print(f'  Pairs evaluated:   {len(all_pairs)}')\n",
    "print(f'  Terms created:     {len(interactions_df.columns)}')\n",
    "print(f'  Terms selected:    {len(selected_terms)} (positive CV R² improvement)')\n",
    "\n",
    "print(f'\\nModel Performance:')\n",
    "print(f'  Best Baseline:  {best_base[\"label\"]}')\n",
    "print(f'    Test R2={best_base[\"test_r2\"]:.4f}  RMSE={best_base[\"test_rmse\"]:.4f}  Gap={best_base[\"gap\"]:.4f}')\n",
    "print(f'  Best Enhanced:  {best_enh[\"label\"]}')\n",
    "print(f'    Test R2={best_enh[\"test_r2\"]:.4f}  RMSE={best_enh[\"test_rmse\"]:.4f}  Gap={best_enh[\"gap\"]:.4f}')\n",
    "print(f'\\n  R² lift from interactions: +{r2_lift:.4f}')\n",
    "print(f'  RMSE reduction:             -{rmse_reduction:.4f}')\n",
    "\n",
    "print(f'\\nStatistically Significant Interaction Terms (OLS, p < 0.05):')\n",
    "if sig_interactions:\n",
    "    for t in sig_interactions:\n",
    "        row = tidy_df[tidy_df['term'] == t].iloc[0]\n",
    "        print(f'  {t}: coef={row[\"estimate\"]:.4f}, p={row[\"p_value\"]:.4e}')\n",
    "else:\n",
    "    print('  None (interactions may still help tree models non-linearly)')\n",
    "\n",
    "print('\\nNext Steps:')\n",
    "print('  1. If gap > 0.08, reduce TOP_N_AUTO in Checkpoint 1 and re-run')\n",
    "print('  2. If R2 lift < 0.005, explore ratio/polynomial interactions more aggressively')\n",
    "print('  3. For production, use the Enhanced Ridge model (best regularization/interpretability balance)')\n",
    "print('=' * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-save-outputs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save key outputs\n",
    "import os\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "compare_df.drop(columns=['model_obj', 'y_test', 'y_pred', 'y_train', 'y_train_pred'], errors='ignore').to_csv('../results/model_comparison.csv', index=False)\n",
    "corr_summary.to_csv('../results/correlation_summary.csv', index=False)\n",
    "candidates_df.to_csv('../results/interaction_candidates.csv', index=False)\n",
    "importance_result.to_csv('../results/interaction_importance.csv', index=False)\n",
    "tidy_df.to_csv('../results/ols_tidy.csv', index=False)\n",
    "if fi_df is not None:\n",
    "    fi_df.to_csv('../results/feature_importance.csv', index=False)\n",
    "\n",
    "print('Results saved to ../results/')\n",
    "print('  model_comparison.csv')\n",
    "print('  correlation_summary.csv')\n",
    "print('  interaction_candidates.csv')\n",
    "print('  interaction_importance.csv')\n",
    "print('  ols_tidy.csv')\n",
    "print('  feature_importance.csv')"
   ]
  }
 ]
}