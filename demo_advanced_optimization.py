#!/usr/bin/env python3
"""
Advanced Optimization Demo
Demonstrates additional statistical analysis and model optimization techniques.
"""

import sys
import os
sys.path.insert(0, 'src')
os.makedirs('results/advanced', exist_ok=True)

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

print("="*80)
print("ðŸš€ ADVANCED ML OPTIMIZATION DEMO")
print("="*80)
print()

# Import our modules
from data_processing import DataProcessor
from correlation_analysis import CorrelationAnalyzer
from interaction_engineering import InteractionEngineer
from model_training import ModelTrainer
from advanced_analysis import AdvancedStatisticalAnalysis
from model_optimization import AdvancedModelOptimizer

# ============================================================================
# STEP 1: Generate Data
# ============================================================================
print("STEP 1: Generating synthetic data...")
print("-" * 80)

np.random.seed(42)
n_samples = 1500

def generate_complex_data(n_samples):
    """Generate data with known patterns for demonstration."""
    data = {
        'feature_1': np.random.randn(n_samples),
        'feature_2': np.random.randn(n_samples),
        'feature_3': np.random.randn(n_samples),
        'feature_4': np.random.exponential(2, n_samples),  # Non-normal
        'feature_5': np.random.randn(n_samples),
        'feature_6': np.random.randn(n_samples),
        'feature_7': np.random.gamma(2, 2, n_samples),  # Non-normal
        'feature_8': np.random.randn(n_samples),
    }

    df = pd.DataFrame(data)

    # Target with complex relationships
    df['target'] = (
        2 * df['feature_1'] +
        3 * df['feature_2'] +
        1.5 * df['feature_3'] +
        # Non-linear effects
        0.5 * df['feature_1'] ** 2 +
        0.3 * np.log(np.abs(df['feature_4']) + 1) +
        # Interaction effects
        2.0 * df['feature_1'] * df['feature_2'] +
        1.5 * df['feature_3'] * df['feature_5'] +
        # Noise
        np.random.randn(n_samples) * 2
    )

    return df

data = generate_complex_data(n_samples)
print(f"âœ… Generated {len(data):,} samples with {len(data.columns)-1} features")
print()

# ============================================================================
# STEP 2: Advanced Statistical Analysis
# ============================================================================
print("STEP 2: Advanced Statistical Analysis...")
print("-" * 80)

analyzer = AdvancedStatisticalAnalysis(data=data, target_col='target')

# 2.1 VIF Analysis
print("\nðŸ“Š 2.1 Variance Inflation Factor (VIF) Analysis")
vif = analyzer.compute_vif(threshold=10.0)
vif.to_csv('results/advanced/vif_analysis.csv', index=False)

# 2.2 Mutual Information
print("\nðŸ“Š 2.2 Mutual Information Analysis")
mi = analyzer.mutual_information_analysis()
mi.to_csv('results/advanced/mutual_information.csv', index=False)

# 2.3 Normality Tests
print("\nðŸ“Š 2.3 Normality Tests")
normality = analyzer.normality_tests()
normality.to_csv('results/advanced/normality_tests.csv', index=False)

print("\nðŸ“Š 2.4 Transformation Recommendations")
transformations = analyzer.recommend_transformations(normality)
print("\nRecommended transformations:")
for feature, recommendation in list(transformations.items())[:5]:
    print(f"  â€¢ {feature}: {recommendation}")

# 2.5 Apply power transformation
print("\nðŸ“Š 2.5 Applying Yeo-Johnson Power Transformation")
data_transformed = analyzer.apply_power_transform(method='yeo-johnson')
print(f"âœ… Transformation complete: {data_transformed.shape}")

# 2.6 Statistical interaction analysis
print("\nðŸ“Š 2.6 Statistical Interaction Analysis")
stat_interactions = analyzer.analyze_feature_interactions_statistical()
stat_interactions.to_csv('results/advanced/statistical_interactions.csv', index=False)

# 2.7 PCA Analysis
print("\nðŸ“Š 2.7 Principal Component Analysis")
pca_results = analyzer.perform_pca_analysis(variance_threshold=0.95)
analyzer.plot_pca_analysis(pca_results, save_path='results/advanced/pca_analysis.png')
plt.close()

print(f"\nâœ… PCA reduced {len(analyzer.numeric_features)} features to "
      f"{pca_results['n_components']} components")
print(f"   Variance explained: {pca_results['explained_variance_ratio'].sum():.1%}")

print("\nâœ… Advanced statistical analysis complete!")
print()

# ============================================================================
# STEP 3: Feature Engineering with Statistical Insights
# ============================================================================
print("STEP 3: Feature Engineering with Statistical Insights...")
print("-" * 80)

# Use original data for modeling (not transformed, to preserve interpretability)
# In practice, you might use transformed data for better model performance

# Identify top features by multiple methods
X = data.drop(columns=['target'])
y = data['target']

feature_comparison = analyzer.feature_selection_comparison(X, y, n_features=8)
feature_comparison.to_csv('results/advanced/feature_selection_comparison.csv', index=False)

print(f"\nâœ… Feature selection comparison complete")
print(f"   Top 5 features by consensus:")
for _, row in feature_comparison.head(5).iterrows():
    print(f"   â€¢ {row['feature']} (rank: {row['avg_rank']:.1f})")

# Create interactions based on statistical analysis
engineer = InteractionEngineer(data=data, target_col='target')

# Use top interaction candidates from statistical analysis
top_stat_interactions = stat_interactions[stat_interactions['promising']].head(5)
interaction_pairs = [(row['feature_1'], row['feature_2'])
                     for _, row in top_stat_interactions.iterrows()]

if interaction_pairs:
    interactions = engineer.batch_create_interactions(
        interaction_pairs,
        interaction_type='multiplicative'
    )
    enhanced_data = engineer.add_interactions_to_data(interactions)
else:
    enhanced_data = data.copy()

print(f"\nâœ… Enhanced dataset: {enhanced_data.shape}")
print()

# ============================================================================
# STEP 4: Advanced Model Optimization
# ============================================================================
print("STEP 4: Advanced Model Optimization...")
print("-" * 80)

# Prepare data
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X_enhanced = enhanced_data.drop(columns=['target'])
y_enhanced = enhanced_data['target']

X_train, X_test, y_train, y_test = train_test_split(
    X_enhanced, y_enhanced, test_size=0.2, random_state=42
)

# Scale features
scaler = StandardScaler()
X_train_scaled = pd.DataFrame(
    scaler.fit_transform(X_train),
    columns=X_train.columns,
    index=X_train.index
)
X_test_scaled = pd.DataFrame(
    scaler.transform(X_test),
    columns=X_test.columns,
    index=X_test.index
)

# Initialize optimizer
optimizer = AdvancedModelOptimizer(
    X_train_scaled, X_test_scaled, y_train, y_test, random_state=42
)

# 4.1 Bayesian Hyperparameter Optimization
print("\nðŸ¤– 4.1 Bayesian Hyperparameter Optimization")
print("    (This may take 1-2 minutes...)")

rf_optimized = optimizer.bayesian_hyperparameter_tuning(
    model_type='random_forest',
    n_iter=30,  # Reduced for demo speed
    cv=3
)

gb_optimized = optimizer.bayesian_hyperparameter_tuning(
    model_type='gradient_boosting',
    n_iter=30,
    cv=3
)

# 4.2 Create Stacking Ensemble
print("\nðŸ¤– 4.2 Creating Stacking Ensemble")
stacking_results = optimizer.create_stacking_ensemble(cv=3)

# 4.3 Create Voting Ensemble
print("\nðŸ¤– 4.3 Creating Voting Ensemble")
voting_results = optimizer.create_voting_ensemble()

# 4.4 Compare All Models
print("\nðŸ¤– 4.4 Model Comparison")
comparison = optimizer.compare_all_models()
comparison.to_csv('results/advanced/advanced_model_comparison.csv', index=False)

# ============================================================================
# STEP 5: Model Interpretability & Diagnostics
# ============================================================================
print("\nSTEP 5: Model Interpretability & Diagnostics...")
print("-" * 80)

# Use best model for detailed analysis
best_model = optimizer.models['stacking']

# 5.1 Learning Curves
print("\nðŸ“ˆ 5.1 Generating Learning Curves")
optimizer.plot_learning_curves(
    best_model,
    model_name='Stacking Ensemble',
    cv=3,
    save_path='results/advanced/learning_curves.png'
)
plt.close()

# 5.2 Permutation Importance
print("\nðŸ“ˆ 5.2 Computing Permutation Importance")
perm_importance = optimizer.compute_permutation_importance(
    best_model,
    model_name='Stacking Ensemble',
    n_repeats=10
)
perm_importance.to_csv('results/advanced/permutation_importance.csv', index=False)

# 5.3 Partial Dependence Plots
print("\nðŸ“ˆ 5.3 Generating Partial Dependence Plots")
top_features = perm_importance.head(3)['feature'].tolist()
feature_indices = [list(X_train_scaled.columns).index(f) for f in top_features]

try:
    optimizer.plot_partial_dependence(
        best_model,
        features=feature_indices,
        model_name='Stacking Ensemble',
        save_path='results/advanced/partial_dependence.png'
    )
    plt.close()
except Exception as e:
    print(f"  âš ï¸  Partial dependence plot failed: {e}")

print("\nâœ… Model interpretability analysis complete!")
print()

# ============================================================================
# STEP 6: Generate Comprehensive Report
# ============================================================================
print("STEP 6: Generating Comprehensive Report...")
print("-" * 80)

# Calculate improvements
baseline_r2 = 0.85  # Approximate baseline from simple linear regression
best_r2 = comparison.iloc[0]['Test_R2']
improvement = ((best_r2 - baseline_r2) / baseline_r2) * 100

summary = {
    'Total Samples': len(data),
    'Training Samples': len(X_train),
    'Test Samples': len(X_test),
    'Original Features': len(data.columns) - 1,
    'Enhanced Features': len(X_enhanced.columns),
    'Interactions Added': len(interaction_pairs) if interaction_pairs else 0,
    'Models Trained': len(optimizer.models),
    'Best Model': comparison.iloc[0]['Model'],
    'Best Test RÂ²': f"{best_r2:.4f}",
    'Best Test RMSE': f"{comparison.iloc[0]['Test_RMSE']:.4f}",
    'Improvement over Baseline': f"{improvement:+.2f}%",
    'PCA Components': pca_results['n_components'],
    'Variance Explained by PCA': f"{pca_results['explained_variance_ratio'].sum():.1%}"
}

summary_df = pd.DataFrame([summary]).T
summary_df.columns = ['Value']
summary_df.to_csv('results/advanced/advanced_optimization_summary.csv')

print("\n" + "="*80)
print("ADVANCED OPTIMIZATION SUMMARY")
print("="*80)
print(summary_df.to_string())
print("="*80)
print()

# Key findings
print("\n" + "="*80)
print("KEY FINDINGS")
print("="*80)

print("\nðŸ“Š Statistical Analysis:")
high_vif = vif[vif['vif'] > 10]
if len(high_vif) > 0:
    print(f"  â€¢ {len(high_vif)} features with high multicollinearity (VIF > 10)")
else:
    print(f"  â€¢ No multicollinearity issues detected")

non_normal = normality[normality['interpretation'] != 'Normal']
print(f"  â€¢ {len(non_normal)} non-normal features (may benefit from transformation)")

print(f"  â€¢ PCA reduced dimensionality: {len(analyzer.numeric_features)} â†’ {pca_results['n_components']} components")

print("\nðŸ¤– Model Performance:")
print(f"  â€¢ Best model: {comparison.iloc[0]['Model']}")
print(f"  â€¢ Test RÂ²: {comparison.iloc[0]['Test_R2']:.4f}")
print(f"  â€¢ Improvement: {improvement:+.2f}% over baseline")
overfitting_gap = comparison.iloc[0]['Overfitting']
if overfitting_gap < 0.05:
    print(f"  â€¢ âœ“ Low overfitting: {overfitting_gap:.4f}")
elif overfitting_gap < 0.10:
    print(f"  â€¢ âš ï¸  Moderate overfitting: {overfitting_gap:.4f}")
else:
    print(f"  â€¢ âŒ High overfitting: {overfitting_gap:.4f}")

print("\nðŸ“ˆ Feature Importance:")
print(f"  â€¢ Top 3 most important features:")
for i, (_, row) in enumerate(perm_importance.head(3).iterrows(), 1):
    print(f"    {i}. {row['feature']}: {row['importance_mean']:.4f}")

print("\n" + "="*80)
print()

# ============================================================================
# COMPLETE
# ============================================================================
print("="*80)
print("âœ… ADVANCED OPTIMIZATION COMPLETE!")
print("="*80)
print("\nAll results saved to results/advanced/:")
print("  ðŸ“Š vif_analysis.csv - Multicollinearity detection")
print("  ðŸ“Š mutual_information.csv - Non-linear relationships")
print("  ðŸ“Š normality_tests.csv - Distribution analysis")
print("  ðŸ“Š statistical_interactions.csv - Interaction candidates")
print("  ðŸ“Š pca_analysis.png - Dimensionality reduction")
print("  ðŸ“Š feature_selection_comparison.csv - Multiple selection methods")
print("  ðŸ¤– advanced_model_comparison.csv - All model results")
print("  ðŸ“ˆ learning_curves.png - Bias-variance analysis")
print("  ðŸ“ˆ permutation_importance.csv - Feature importance")
print("  ðŸ“ˆ partial_dependence.png - Feature effects")
print("  ðŸ“‹ advanced_optimization_summary.csv - Complete summary")
print()
print("Next steps:")
print("  â€¢ Review results in results/advanced/")
print("  â€¢ Apply these techniques to your own data")
print("  â€¢ Combine with original framework for complete pipeline")
print("="*80)
